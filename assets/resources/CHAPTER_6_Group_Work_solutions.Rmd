---
title: "Chapter 5 Group Work"
subtitle: "SOLUTIONS"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1

Let $X$ have the pdf $f(x) = 4x^3, \ \ \  0 < x < 1$.  Find the pdf of $Y = X^2$ using the CDF technique. Verify that it is a valid pdf. 

### SOLUTION

$$
\begin{aligned}
G(y) &= P(Y \leq y) \\
&= P(X^2 \leq y) \\
&= P(X \leq \sqrt{y}) \\
&= \int_0^{\sqrt{y}}f(x)dx \\
&= \int_0^{\sqrt{y}} 4x^3dx \\
&= y^2
\end{aligned}
$$

Then, the pdf of $Y$ is given by $$g(y) = G'(y) = 2y, \ \ \ 0 < y < 1$$

This is a valid pdf because $\int_0^12ydy = 1$.

## Problem 2

Let  $X$ be a random variable with a $\Gamma(\alpha = 3, \theta = 2)$ distribution.  What is the distribution (including the support) of $Y=\sqrt{X}$? Use the change of variable technique.

### SOLUTION

$X \sim \Gamma(\alpha = 3, \theta = 2) \implies f(x) = \frac{1}{\Gamma(3)2^3}x^{3-1}e^{-x/2} = \frac{1}{16}x^2e^{-x/2}$

$Y = \sqrt{X} \implies X = Y^2 \equiv v(y),$ and $v'(y) = 2y$

Using the change of variable technique, 

$$
\begin{aligned}
g(y) &= f(v(y))|v'(y)| \\
&= \frac{1}{16}(y^2)^2e^{-y^2/2}|2y| \\
&= \frac{y^5}{8}e^{-y^2/2}, \ \ \ 0 < y < \infty
\end{aligned}
$$

## Problem 3

The Cauchy distribution is in the text - it is a distribution with such "heavy tails" that the expected value is undefined. It's useful for modeling situations with the possibility of extreme outcomes. The cdf of the Cauchy distribution is: $$F(x) = \frac{1}{\pi}\left(arctan(x) + \frac{\pi}{2}\right) \ \ \ -\infty < x < \infty$$ Describe how to simulate observations from the Cauchy distribution.

### SOLUTION 

Need to find $F^{-1}(x)$, the inverse of the cdf:

$$\begin{aligned}
y &= \frac{1}{\pi}\left(arctan(x) + \frac{\pi}{2}\right) \\
\pi y&= arctan(x) + \frac{\pi}{2} \\
\pi y - \frac{\pi}{2} &= arctan(x) \\
tan\left(\pi y - \frac{\pi}{2}\right) &= x  
\end{aligned}$$

Therefore, you can simulate a large number of $y$ values from the uniform distribution using the `runif()` function, and then calculate the x's from the y's according to the formula above.

## Problem 4

Prove that $\mu_Y = \sum_{i = 1}^na_i\mu_i$ when $Y = \sum_{i = 1}^na_ix_i$ (from Theorem 5.3-2)

### SOLUTION

$$
\begin{aligned}
\mu_Y = E(Y) &= E(\sum_{i = 1}^na_ix_i)\\
&= \sum_{i = 1}^n E(a_ix_i) \\
&= \sum_{i = 1}^n a_i E(x_i) \\
&= \sum_{i = 1}^na_i\mu_i
\end{aligned}
$$

## Problem 5

Let $X_1, X_2, \dots X_n$ be a random sample from a distribution with mean $\mu$ and variance $\sigma^2$. Find the mean and the variance of the sample mean. That is, find $E(\overline{X})$ and $V(\overline{X})$. Will the variance of the sample mean increase or decrease as $n$ increases? 

### SOLUTION

$\overline{X} = \frac{1}{n}\sum_{i = 1}^n X_i$

$$\begin{aligned}
E(\overline{X}) &= E(\frac{1}{n}\sum_{i = 1}^n X_i)) \\
&= \frac{1}{n}\sum_{i = 1}^n E(X_i)\\
&= \frac{1}{n}\sum_{i = 1}^n \mu \\
&= \frac{1}{n} n\mu \\
&= \mu
\end{aligned}$$

$$\begin{aligned}
V(\overline{X}) &= V(\frac{1}{n}\sum_{i = 1}^n X_i)) \\
&= \frac{1}{n^2}\sum_{i = 1}^n V(X_i)\\
&= \frac{1}{n^2}\sum_{i = 1}^n \sigma^2 \\
&= \frac{1}{n^2} n\sigma^2 \\
&= \frac{\sigma^2}{n}
\end{aligned}$$

The $n$ on the denominator indicates that the variance decreases as the sample size increases

## Problem 6

If $X$ and $Y$ have correlation $\rho$, and $\rho < 0,$ then which of the following is true? Justify your answer

  A) $\sigma^2_{X + Y} < \sigma_X^2 + \sigma_Y^2$
  A) $\sigma^2_{X + Y} > \sigma_X^2 + \sigma_Y^2$
  A) $\sigma^2_{X + Y} = \sigma_X^2 + \sigma_Y^2$
    
### SOLUTION

The answer is A.

To show this, first note that $\sigma^2_{X + Y} = V(X + Y)$, $\sigma^2_X = V(X)$, and $\sigma^2_Y = V(Y)$

$$\begin{aligned}
\sigma^2_{X + Y} &= V(X + Y) \\
&= V(X) + V(Y) + 2Cov(X,Y) \\
&= V(X) + V(Y) + 2\rho_{xy}\sigma_x\sigma_y \\
&< V(X) + V(Y)
\end{aligned}$$

The last step is true since $\rho_{xy} < 0$.
    
## Problem 7

Let $X_1$ and $X_2$ be independent random variables with pdfs $f_1(x_1) = 2x_1, 0 <x_1<1$ and $f_2(x_2) = 4x_2^3, 0 <x_2<1$ respectively. Compute
    
  a. $P(0.5 < X_1 < 1, 0.4 < X_2 < 0.8)$
  b. $E(X_1^2X_2^3)$
    
### SOLUTION

#### Part a

Because of independence, 

$$
\begin{aligned}
P(0.5 < X_1 < 1, 0.4 < X_2 < 0.8) &= P(0.5 < X_1 < 1)P(0.4 < X_2 < 0.8) \\
&= \int_{0.5}^1 2x_1dx_1 \int_{0.4}^{0.8}4x_2^3dx_2 \\
&= (0.75)(0.384) \\
&= 0.288
\end{aligned}
$$

#### Part b

Because of independence, 

$$\begin{aligned}
E(X_1^2X_2^3) &= E(X_1^2)E(X_2^3)\\
&= \int_0^1x_1^22x_1dx_1 \int_0^1x_2^34x_2^3dx_2 \\
&= \int_0^12x_1^3dx_1 \int_0^14x_2^6dx_2 \\
&= \frac{1}{2}\cdot \frac{4}{7} \\
&= \frac{2}{7}
\end{aligned}$$

## Problem 8

(5.5-3) Let $X$ equal the widest diameter (in millimeters) of the fetal head measured between the 16th and 25th weeks of pregnancy. Assume that the distribution of $X$
is $N(46.58, 40.96)$. Let $\overline{X}$ be the sample mean of a random sample of $n = 16$ observations of $X$.  
  a. Give the values of $E(\overline{X})$ and $V(\overline{X})$
  b. Find $P(44.4\leq\overline{X}\leq48.98)$ (using R).
  
### SOLUTION Part a

$E(\overline{X}) = \mu = 46.58$
$V(\overline{X}) = \sigma^2/n = 40.96/16 = 2.56$

### SOLUTION Part b

```{r}
pnorm(48.98, mean = 46.58, sd = sqrt(2.56)) - pnorm(44.4, mean = 46.58, sd = sqrt(2.56))
```

    
## Problem 9

(5.5-8) Let $X$ denote the wing length in millimeters of a male gallinule and $Y$ the wing length in millimeters of a female gallinule. Assume that $X$ is $N(184.09,39.37)$ and $Y$ is $N(171.93, 50.88)$ and that $X$ and $Y$ are independent. If a male and a female gallinule are captured, what is the probability that $X$ is greater than $Y$? That is, what is $P(X > Y)$? *Hint: finding the distribution of $X-Y$ will be useful*

### SOLUTION

$X - Y$ is a linear combination of independent normal random variables, there for it's distribution is normal with mean $\sum_{i = 1}^2a_i\mu_i$ and variance $\sum_{i = 1}^2a_i^2\sigma^2_i$. Since $a_1 = 1$ and $a_2 = -1$, we have $X - Y \sim N(\mu_1 - \mu_2, \sigma_1^2 + \sigma_2^2) = N(184.09 - 171.93, 39.37 + 50.88) = N(12.16, 90.25)$. Therefore, we can find the probability

$$P(X > Y) = P(X - Y > 0)$$
with the following code:

```{r}
pnorm(0, 12.16, sqrt(90.25), lower.tail = FALSE)
```


## Problem 10

(5.5-15) Let the distribution of $T$ be $t(17)$. Using R, find

  a. $t_{0.01}(17)$ *Hint: find the cut-off value of the $t(17)$ distribution such that 0.01 falls in the upper tail.*
  b. $t_{0.95}(17)$
  c. $P(-1.740 \leq T \leq 1.740)$

### SOLUTION 

```{r}
#part a
qt(.99, 17)

#part b
qt(.05, 17)

#part c
pt(1.74, 17) - pt(-1.74, 17)
```

## Problem 11 

(5.6-7) Let $X$ equal the maximal oxygen intake of a human on a treadmill, where the measurements are in milliliters of oxygen per minute per kilogram of weight. Assume that, for a particular population, the mean of $X$ is $\mu = 54.030$ and the standard deviation is $\sigma = 5.8$. Let $X$ be the sample mean of a random sample of size $n = 47$. Find $P(52.761 \leq X \leq 54.453)$, approximately.

### SOLUTION

$\overline{X} \sim N(54.030, 5.8^2/47)$

```{r}
pnorm(54.453, 54.030, 5.8/sqrt(47)) - pnorm(52.761, 54.030, 5.8/sqrt(47))
```


## Problem 12

$X_1, X_2, ...X_{20}$ be a random sample of size 20 from the uniform distribution on the interval (0,1). What is the approximate distribution of the sample total, $Y = X_1 + X_2 + ... + X_{20}$? What is the approximate probability the total is less than 9.1? Between 8.5 and 11.7?

### SOLUTION

$X_i \sim U(0,1) \implies E(X_i) = \frac{(1-0)}{2} \equiv \mu, \ \ \ V(X_i) = \frac{(1-0)^2}{12} = \frac{1}{12} \equiv \sigma^2$ by properties of the uniform distribution. Then, by the Central Limit Theorem, we know the sample total $Y = \sum_{i = 1}^{20} X_i \approx N(n\mu, n\sigma^2)$. Therefore, $Y \approx N(10, 20/12)$

```{r}
pnorm(9.1, 10, sqrt(20/12))

pnorm(11.7, 10, sqrt(20/12)) - pnorm(8.5, 10, sqrt(20/12))
```


## Problem 13

(5.6-13) Let $X_1$, $X_2$, $X_3$, $X_4$ represent the random times in days needed to complete four steps of a project. These times are independent and have gamma distributions with common $\theta = 2$ and $\alpha_1 = 3, \alpha_2 = 2, \alpha_3 = 5, \alpha_4 = 3$, respectively. One step must be completed before the next can be started. Let $Y = \sum_{i = 1}^4X_i$ equal the total time needed to complete the project.
    
  a) Find an integral that represents $P(Y \leq 25)$. You can use the fact that the sum of gamma random variables is also gamma, with $\theta = \theta$ and $\alpha =  \sum_{i = 1}^4\alpha_i$
  b) Using R, show that the integral in part a evaluates to 0.481. *Hint: use `pgamma()` and recall `shape` = $\alpha$ and `scale` = $\theta$ 
  c) Using a normal distribution, approximate $P(Y \leq 25)$. *Hint: you should first determine $E(Y)$ and $V(Y)$.* Is this approach justified? 
    
### SOLUTION part a

$\sum_{i = 1}^4 \alpha_i = 3 + 2 + 5 + 3 = 13$, so $Y = \sum_{i = 1}^4X_i \sim \Gamma(\theta = 2, \alpha = 13)$. Therefore the pdf of $Y$ is given by
$$f(y) = \frac{1}{\Gamma(13)2^{13}}y^{13 - 1}e^{-y/2}, \ \ \ 0 < y < \infty$$ and $$P(Y \leq 25) = \int_0^{25}\frac{1}{\Gamma(13)2^{13}}y^{13 - 1}e^{-y/2}dy$$

### SOLUTION part b

```{r}
pgamma(25, shape = 13, scale = 2)
```

### SOLUTION part c

Note that each $X_i \sim \Gamma(\theta, \alpha_i)$ has $\mu_i = E(X_i) = \alpha_i\theta$ and $\sigma_i^2 = V(X_i) = \alpha_i\theta^2$. Therefore, by Theorem 5.3-2, $E(Y) = E(\sum_{i = 1}^4X_i) = \sum_{i = 1}^4E(X_i)=\sum_{i = 1}^4\mu_i = \sum_{i = 1}^4 \alpha_i\theta = \theta \sum_{i = 1}^4 \alpha_i = 2(13) = 26$ and $V(Y) = V(\sum_{i = 1}^4X_i) = \sum_{i = 1}^4V(X_i)=\sum_{i = 1}^4\sigma_i^2 = \sum_{i = 1}^4 \alpha_i\theta^2 = \theta^2 \sum_{i = 1}^4 \alpha_i = 4(13) = 52$

Therefore, by the CLT, an approximation pf $P(Y \leq 25)$ can be found by:

```{r}
pnorm(25, 26, sqrt(52))
```

This approximation is not great because we only have $n = 4$, so using the CLT may not be justified. 