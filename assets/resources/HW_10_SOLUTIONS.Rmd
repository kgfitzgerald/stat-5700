---
title: "HW 10 SOLUTIONS"
format: pdf
---

# Practice Problems

## 5.45

No. Counterexample:
$$
P(Y_1 = 2, Y_2 = 2) = 0 \neq P(Y_1 = 2)P(Y_2 = 2) = \left(\frac{1}{9}\right)\left(\frac{1}{9}\right).
$$

## 5.49

Note that
$$
f_1(y_1) = \int_0^{y_1} 3y_1 \, dy_2 = 3y_1^2, \qquad 0 \le y_1 \le 1,
$$
and
$$
f_2(y_2) = \int_{y_2}^1 3y_1 \, dy_1 = \frac{3}{2}\left(1 - y_2^2\right), \qquad 0 \le y_2 \le 1.
$$

Thus,
$$
f(y_1, y_2) \neq f_1(y_1) f_2(y_2),
$$
so that $Y_1$ and $Y_2$ are dependent.

## 5.53 

The ranges of $y_1$ and $y_2$ depend on each other so $Y_1$ and $Y_2$ cannot be independent.

## 5.63

$$
P(Y_1 > Y_2,\, Y_1 < 2Y_2)
= \int_0^\infty \int_{y_1/2}^{y_1} e^{-(y_1 + y_2)} \, dy_2 \, dy_1
= \frac{1}{6},
$$

and
$$
P(Y_1 < 2Y_2)
= \int_0^\infty \int_{y_1/2}^{\infty} e^{-(y_1 + y_2)} \, dy_2 \, dy_1
= \frac{2}{3}.
$$

So,
$$
P(Y_1 > Y_2 \mid Y_1 < 2Y_2) = \frac{1}{4}.
$$

## 5.77

Assume uniform distributions for the call times over the 1-hour period.

**(a)**  
$$
P(Y_1 \le 1/2,\, Y_2 \le 1/2)
= P(Y_1 \le 1/2)P(Y_2 \le 1/2)
= (1/2)(1/2)
= 1/4.
$$

**(b)**  
Note that 5 minutes $= 1/12$ hour. To find $P(|Y_1 - Y_2| \le 1/12)$, we break the region into three parts:
$$
\begin{aligned}
P(|Y_1 - Y_2| \le 1/12)
&= \int_0^{1/12} \int_0^{y_1 + 1/12} 1 \, dy_2 \, dy_1 \\
&\quad + \int_{1/12}^{11/12} \int_{y_1 - 1/12}^{y_1 + 1/12} 1 \, dy_2 \, dy_1 \\
&\quad + \int_{11/12}^{1} \int_{y_1 - 1/12}^{1} 1 \, dy_2 \, dy_1 \\
&= \frac{23}{144}.
\end{aligned}
$$

## 5.81

Since $Y_1$ and $Y_2$ are independent,  
$$
E\!\left(\frac{Y_2}{Y_1}\right) = E(Y_2)\,E\!\left(\frac{1}{Y_1}\right).
$$

Thus, using the marginal densities found in Exercise 5.61,
$$
\begin{aligned}
E\!\left(\frac{Y_2}{Y_1}\right)
&= E(Y_2)\,E\!\left(\frac{1}{Y_1}\right) \\
&= \frac{1}{2} \int_0^\infty y_2 e^{-y_2/2} \, dy_2
\left[ \frac{1}{4} \int_0^\infty e^{-y_1/2} \, dy_1 \right] \\
&= 2\left(\frac{1}{2}\right) \\
&= 1.
\end{aligned}
$$


## 5.87

**(a)**  
$$
E(Y_1 + Y_2) = E(Y_1) + E(Y_2) = \nu_1 + \nu_2.
$$

**(b)**  
By independence,
$$
\operatorname{Var}(Y_1 + Y_2)
= \operatorname{Var}(Y_1) + \operatorname{Var}(Y_2)
= 2\nu_1 + 2\nu_2.
$$

## 5.89

$$
\begin{aligned}
\operatorname{Cov}(Y_1, Y_2)
&= E(Y_1 Y_2) - E(Y_1)E(Y_2) \\
&= \sum_{y_1} \sum_{y_2} y_1 y_2 \, p(y_1, y_2) - \left[2\left(\frac{1}{3}\right)\right]^2 \\
&= \frac{2}{9} - \frac{4}{9} \\
&= -\frac{2}{9}.
\end{aligned}
$$

As the value of $Y_1$ increases, the value of $Y_2$ tends to decrease.


## 5.93

a. From Ex. 5.55 and 5.79, $E(Y_1Y_2) = 0$ and $E(Y_1) = 0$. So,

$$
\text{Cov}(Y_1, Y_2) = E(Y_1Y_2) - E(Y_1)E(Y_2) = 0 - 0 \cdot E(Y_2) = 0.
$$

b. $Y_1$ and $Y_2$ are dependent.

c. Since $\text{Cov}(Y_1, Y_2) = 0$, $\rho = 0$.

d. If $\text{Cov}(Y_1, Y_2) = 0$, $Y_1$ and $Y_2$ are not necessarily independent.

## 5.95

Note that the marginal distributions for $Y_1$ and $Y_2$ are

$$
\begin{array}{c|ccc}
y_1 & -1 & 0 & 1 \\ \hline
p_1(y_1) & \tfrac{1}{3} & \tfrac{1}{3} & \tfrac{1}{3}
\end{array}
\qquad
\begin{array}{c|cc}
y_2 & 0 & 1 \\ \hline
p_2(y_2) & \tfrac{2}{3} & \tfrac{1}{3}
\end{array}
$$

So, $Y_1$ and $Y_2$ are not independent since
$$
p(-1,0) \neq p_1(-1)p_2(0).
$$
However, $E(Y_1) = 0$ and
$$
\begin{aligned}
E(Y_1Y_2)
&= (-1)(0)\left(\tfrac{1}{3}\right)
 + (0)(1)\left(\tfrac{1}{3}\right)
 + (1)(0)\left(\tfrac{1}{3}\right) \\
&= 0,
\end{aligned}
$$
so
$$
\operatorname{Cov}(Y_1, Y_2) = 0.
$$



# Submitted Problems 

## 5.48

Dependent. For example,
$$
P(Y_1 = 0, Y_2 = 0) \neq P(Y_1 = 0)\,P(Y_2 = 0).
$$


## 5.52

Note that $f(y_1,y_2)$ can be factored and the ranges of $y_1$ and $y_2$ do not depend on each other so by Theorem 5.5 $Y_1$ and $Y_2$ are independent.

## 5.60

From Exercise 5.36,
$$
f_1(y_1) = y_1 + 1/2, \quad 0 \le y_1 \le 1,
$$
and
$$
f_2(y_2) = y_2 + 1/2, \quad 0 \le y_2 \le 1.
$$
But,
$$
f(y_1, y_2) \neq f_1(y_1) f_2(y_2),
$$
so $Y_1$ and $Y_2$ are dependent.

## 5.64

$P(Y_1 > Y_2,\ Y_1 < 2Y_2) = \int_{1/2}^{1} \int_{0}^{y_1} 1 \, dy_2 \, dy_1 = \frac{1}{4}$

$P(Y_1 < 2Y_2) = 1 - P(Y_1 \geq 2Y_2) = 1 - \int_{0}^{1} \int_{0}^{y_1/2} 1 \, dy_2 \, dy_1 = \frac{3}{4}$

So, $P(Y_1 > Y_2 \mid Y_1 < 2Y_2) = \frac{1}{3}$

## 5.76

From Ex. 5.52, we found that $Y_1$ and $Y_2$ are independent. So,

**a.** 
$$
E(Y_1) = \int_{0}^{1} 2y_1^2 \, dy_1 = \frac{2}{3}.
$$

**b.**
$$
E(Y_1^2) = \int_{0}^{1} 2y_1^3 \, dy_1 = \frac{2}{4}, \text{ so } V(Y_1) = \frac{2}{4} - \frac{4}{9} = \frac{1}{18}.
$$

**c.**
$$
E(Y_1 - Y_2) = E(Y_1) - E(Y_2) = 0.
$$

## 5.80

From Ex. 5.36, $f_1(y_1) = y_1 + \frac{1}{2}, \; 0 \le y_1 \le 1,$ and $f_2(y_2) = y_2 + \frac{1}{2}, \; 0 \le y_2 \le 1.$ Thus,

$$
E(Y_1) = \frac{7}{12} \text{ and } E(Y_2) = \frac{7}{12}.
$$

So,
$$
E(30Y_1 + 25Y_2) = 30\left(\frac{7}{12}\right) + 25\left(\frac{7}{12}\right) = 32.08.
$$

## 5.92

From Ex. 5.77, $E(Y_1) = \frac{1}{4}$ and $E(Y_2) = \frac{1}{2}.$

$$
E(Y_1 Y_2) = \int_{0}^{1} \int_{0}^{y_2} 6 y_1 y_2 (1 - y_2) \, dy_1 \, dy_2 = \frac{3}{20}.
$$

So,
$$
\text{Cov}(Y_1, Y_2) = E(Y_1 Y_2) - E(Y_1)E(Y_2) = \frac{3}{20} - \frac{1}{8} = \frac{1}{40} \text{ as expected since } Y_1 \text{ and } Y_2 \text{ are dependent.}
$$

## 5.94

**a.**
$$
\text{Cov}(U_1, U_2) = E[(Y_1 + Y_2)(Y_1 - Y_2)] - E(Y_1 + Y_2)E(Y_1 - Y_2)
$$
$$
= E(Y_1^2) - E(Y_2^2) - [E(Y_1)]^2 - [E(Y_2)]^2
$$
$$
= (\sigma_1^2 + \mu_1^2) - (\sigma_2^2 + \mu_2^2) - (\mu_1^2 - \mu_2^2) = \sigma_1^2 - \sigma_2^2.
$$

**b.** Since $V(U_1) = V(U_2) = \sigma_1^2 + \sigma_2^2$ (Y₁ and Y₂ are uncorrelated),
$$
\rho = \frac{\sigma_1^2 - \sigma_2^2}{\sigma_1^2 + \sigma_2^2}.
$$

**c.** If $\sigma_1^2 = \sigma_2^2$, $U_1$ and $U_2$ are uncorrelated.

# Additional Problems

## Problem 1 

Let $X$ and $Y$ be independent random variables with $\mu_X = 1$, $\sigma_X = 10$, $\mu_Y = 2$, and $\sigma_Y = 4$. Compute the mean and standard deviation for:

**a. $X + Y$**
$$
\mu = \mu_X + \mu_Y = 1 + 2 = 3, \quad
\sigma = \sqrt{\sigma_X^2 + \sigma_Y^2} = \sqrt{10^2 + 4^2} = \sqrt{116} \approx 10.77.
$$

**b. $X - Y$**
$$
\mu = \mu_X - \mu_Y = 1 - 2 = -1, \quad
\sigma = \sqrt{\sigma_X^2 + \sigma_Y^2} = \sqrt{10^2 + 4^2} = \sqrt{116} \approx 10.77.
$$

**c. $X + 4Y$**
$$
\mu = \mu_X + 4\mu_Y = 1 + 4(2) = 9, \quad
\sigma = \sqrt{\sigma_X^2 + (4^2)\sigma_Y^2} = \sqrt{10^2 + 16(4^2)} = \sqrt{100 + 256} = \sqrt{356} \approx 18.87.
$$

**d. $2X - 5Y$**
$$
\mu = 2\mu_X - 5\mu_Y = 2(1) - 5(2) = -8, \quad
\sigma = \sqrt{(2^2)\sigma_X^2 + (5^2)\sigma_Y^2} = \sqrt{4(10^2) + 25(4^2)} = \sqrt{400 + 400} = \sqrt{800} \approx 28.28.
$$

## Problem 2

We are given:
$$
V(X) = 5, \; V(Y) = 4, \; \text{and } \text{Cov}(X,Y) = -2.
$$

Recall:
$$
\text{Cov}(aX + bY, cX + dY) = ac \, V(X) + bd \, V(Y) + (ad + bc) \, \text{Cov}(X,Y).
$$

---

**a. $\text{Cov}(X + Y, X - Y)$**
Here, $a = 1, b = 1, c = 1, d = -1$:
$$
\text{Cov}(X + Y, X - Y) = (1)(1)(5) + (1)(-1)(4) + (1)(-1) + (1)(1)
$$
$$
= 5 - 4 + ( -1 + 1)(-2) = 5 - 4 + 0 = 1.
$$

---

**b. $\text{Cov}(X - Y, -2X + 5Y)$**
Here, $a = 1, b = -1, c = -2, d = 5$:
$$
\text{Cov}(X - Y, -2X + 5Y) = (1)(-2)(5) + (-1)(5)(4) + (1)(5) + (-1)(-2)
$$
$$
= -10 - 20 + (5 + 2)(-2) = -10 - 20 - 14 = -44.
$$

---

**c. $\text{Corr}(X - Y, -2X + 5Y)$**
First compute variances:

For $X - Y$:
$$
V(X - Y) = V(X) + V(Y) - 2\text{Cov}(X,Y) = 5 + 4 - 2(-2) = 9 + 4 = 13.
$$

For $-2X + 5Y$:
$$
V(-2X + 5Y) = (-2)^2 V(X) + (5)^2 V(Y) + 2(-2)(5)\text{Cov}(X,Y)
$$
$$
= 4(5) + 25(4) + 2(-10)(-2) = 20 + 100 + 40 = 160.
$$

So:
$$
\text{Corr}(X - Y, -2X + 5Y) = \frac{\text{Cov}(X - Y, -2X + 5Y)}{\sqrt{V(X - Y)} \sqrt{V(-2X + 5Y)}}
$$
$$
= \frac{-44}{\sqrt{13} \cdot \sqrt{160}} = \frac{-44}{\sqrt{2080}} \approx \frac{-44}{45.6} \approx -0.965.
$$

## Problem 3

We are given that $\rho_{XY} \neq 0$, and $W = bX$ where $b > 0$. Show that $\rho_{WY} = \rho_{XY}$.

Recall the correlation formula:
$$
\rho_{WY} = \frac{\text{Cov}(W,Y)}{\sqrt{V(W)} \sqrt{V(Y)}}.
$$

Since $W = bX$:
$$
\text{Cov}(W,Y) = \text{Cov}(bX,Y) = b \, \text{Cov}(X,Y).
$$

Also:
$$
V(W) = V(bX) = b^2 V(X).
$$

So:
$$
\rho_{WY} = \frac{b \, \text{Cov}(X,Y)}{\sqrt{b^2 V(X)} \sqrt{V(Y)}} = \frac{b \, \text{Cov}(X,Y)}{b \sqrt{V(X)} \sqrt{V(Y)}} = \frac{\text{Cov}(X,Y)}{\sqrt{V(X)} \sqrt{V(Y)}} = \rho_{XY}.
$$

Thus:
$$
\rho_{WY} = \rho_{XY}.
$$